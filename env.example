# Environment Configuration
# Copy this file to .env.local for local development or .env for production

# LLM Configuration
LLM_PROVIDER=ollama
# Options: ollama, openrouter, lmstudio

# For Ollama (local development)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# For LMStudio (local OpenAI-compatible)
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=qwen/qwen3-4b-thinking-2507

# For OpenRouter API (production)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# Local Server Configuration
PORT=3000
CHAT_SERVER=http://localhost:3000

# Honcho Configuration
HONCHO_BASE_URL=http://localhost:8000
HONCHO_API_KEY=your_honcho_api_key_here
